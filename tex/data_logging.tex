
The data generated by the Nodes are in many different types, such as integer, floating point, string, list, tuple, and other custom object types. In addition, objects such as the CBLA Learner are continuously expanding and its hierarchy gets deeper over time. Moreover, each type of data packet gets generated at different non-constant time cycles. This makes simply saving them in a table or a relational database impractical. In our implementation, a simple key-value type NoSQL database based on Python's shelve\footnote{Python shelve documentation: \url{https://docs.python.org/3.4/library/shelve.html}} module was used. In shelve, the data are stored as serialized Python objects using the pickle\footnote{Python pickle documentation: \url{https://docs.python.org/3.4/library/pickle.html}} module. This type of database gives the flexibility of storing an assortment of data types without the need to predefine them. However, since the data are not stored as plain text, a special script is required to extract the data in the desired formats for offline analysis. 


\section{Database Structure}

Each time the CBLA System is run, a new shelve database is created for the session. This is to ensure that data from previous sessions does not get corrupted accidentally. In addition, in order to restart from a previous session, one only has to remove the database files of the succeeding sessions. If there are previous sessions, the CBLA system will access an index file to locate the database file of the most recent session and retrieve information regarding the previous state of the system. Information about the start time, end time, and the configurations of the system can also be found in the index file. \Cref{fig:data_structure} illustrates the structure of the database created by the Data Logger. 

\begin{figure}
	\dirtree{%
		.1 Session 1.
		.2 Node A.
		.3 Info Type.
		.4 S names: (IR Sensor 0, IR Sensor 1, ...).
		.4 M names: (LED, Vibration Motor, ...).
		.4 CBLA State.
		.5 Step Count: 245.
		.5 Experts.
		.6 Expert IDs: (0, 3, ...).
		.6 Mean Errors: (0.1, 0.001, ...).
		.6 Prediction Models: (linreg obj 1, linreg obj 2, ...).
		.6 ....
		.5 Robot: reflex\_robot obj.	
		.5 ....
		.4 ....
		.3 Packet Type.
		.4 t = 1.00s.
		.5 packet (t = 1.001s).
		.6 S: (10, 1024, 20, ...).
		.6 M: (0, 140, 255, ...).
		.6 Relative Action Value: 10.
		.6 ....
		.5 packet (t = 1.002s).
		.5 ....
		.4 t = 1.01s.
		.4 ....
		.2 Node B.
		.2 ....
	}
	\dirtree{%
		.1 Session 2.
	}
	\dirtree{%
		.1 ....
	}
	\caption[Structure of the database created by Data Logger]{Example showing the structure of the database created by the Data Logger .}
	\label{fig:data_structure}
\end{figure}

For each Node, there are two main types of data: Packet Type and Info Type. Each Packet Type data has a timestamp which indicates when the data block is generated. These data blocks are generated continuously and they must all be stored. They contain information about the sensor readings, actuator outputs, and the internal parameters of the CBLA Engine such as its current mean error, number of regions, and relative action values. Packet Type data are mainly used for secondary analysis purposes. On the other hand, Info Type data describes the system and does not have a timestamp. When new versions of Info Type data arrive, the old ones can be overwritten. Information like the names and order of the sensor and actuator variables, which do not change over the runtime of the system, are Info Type data. In addition, the state of the CBLA Engine, which is needed for recovering from a later time, is also an Info Type data. Over the long term, while it is desirable for the state to be saved frequently, the size of the data that describes it would become too large to allow multiple copies of it to be stored. By saving it as an Info Type, the old version can simply be overwritten by a newer version.

\FloatBarrier
\section{Data Logging Process}

Due to the large amount of data that are being generated at high frequencies, saving each block of data directly would require too much time and introduce significant lag in the system. Therefore, a multi-stage process as illustrated in \Cref{fig:DataLogger process} is used to improve data logging efficiency.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1.0 \textwidth]{"fig/interactive control system/DataLogger process"}
	\caption[Flowchart of the data logging process]{Flowchart of the data logging process from data generation to writing to the disk.}
	\label{fig:DataLogger process}
\end{figure}

Packet Type data blocks are being generated at a rate of approximately one block every 20 milliseconds per Node. This is relatively high given the hundreds of Nodes that a typical CBLA system has. On the other hand, Info Type data are being generated at a much slower rate at roughly one data block every two minutes per Node. Both types of data blocks would first get enqueued onto the Data Logger before being transferred to the Data Saver which is the module responsible for writing the data into the shelve database. This step is necessary to avoid slowing down the CBLA Nodes because Data Saver is a separate process, and transferring data to another Process takes significantly longer time than to another thread. 

For the higher frequency Packet Type data blocks, instead of directly being sent to the Data Saver, they are first packed in Packet Arrays. This process decreases the total transfer time to the Data Saver by drastically lowering the number of enqueue calls which have non-trivial overhead. There is one Packet Array for each Node and they get enqueued to the Data Saver periodically. 

The Data Saver is implemented as a process in order to avoid the GIL limitation imposed by Python \cite{Beazley2010}. This enables the system to make use of the parallel computing capability of a multi-core computer. Although transferring data to a process takes longer as it requires the copying of the actual data rather than just the pointers, it is still faster than writing the data onto the disk. Therefore, it is still more efficient to move the data and let a separate process load the data onto the database.

Nevertheless, depending on the number of Nodes in the system, there are situations when data are indeed being generated at a rate faster than it can be stored. Eventually, over the long term, the memory of the host computer will be full and the program will crash. In addition, a long wait time between data generation and data storage means that if the program crashes unexpectedly, a large amount of data would be lost. To avoid crashing and the loss of data, once the length of the queue has reached a certain threshold, a clean-up procedure is activated. It momentarily pauses all other threads, and allocates all the processing power for the purposes of data storage. Practically, this process would only takes around 50 to 70 milliseconds. Since it happens only once every few minutes, it is generally not noticeable by human viewer. In fact, the reason why it only takes so little time is because, by pausing all other threads, it eliminates the overhead of thread switching. This process ensures that the CBLA system can operate as long as there is sufficient storage space on the hard drive of the host computer. 