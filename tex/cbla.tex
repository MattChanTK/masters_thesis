\chapter[Curiosity-Based Learning Algorithm]
{Curiosity-Based Learning Algorithm\footnote{An early version of this chapter has been published at IROS 2015 \cite{Chan2015} }} 
\label{chap:cbla}

In this chapter, our approach for adapting and generalizing the IAC algorithm \cite{Oudeyer2007} for implementation of the Curiosity-Based Learning Algorithm (CBLA) on a distributed kinetic sculpture is presented. This includes introducing additional parameters, modifications to action selection approaches, and the integration of multiple independent learning agents in a distributed network. 


\section{CBLA Engine}

An interactive art sculpture that implements the IAC must first be able to sense the consequences of its actions, similar to the human capability for proprioception. These sensors allow the sculpture to both detect its own actions, and the actions of occupants on its embodiment. For example, an accelerometer that is mounted on a mechanism senses both when the mechanism is activated by its actuators, and also when it is touched by a person during interaction. The sculpture can only improve the estimate of its own dynamics through comparing the proprioceptive feedbacks with a feed-forward model. For a human being, this capability is implemented through a neural mechanism known as efferent copy \cite{Bridgeman2007}. For example, human eyes are constantly moving while a stable image is reconstructed using the efferent copy. However, the efferent copy can be deceiving when the external environment is disturbed and as a result, conflicts with the predicted model. For instance, a stationary image will appear to be moving when the eye is pressed \cite{Bridgeman2007}. However, if the disturbance is permanent, over time the efferent copy will be updated to reflect the new conditions, and an accurate model is once again available for prediction. Hence, not only is an accurate model of an agent's own dynamics difficult to obtain; such a model might change over the life of the installation due to wear and tear and interaction with its surrounding environments. Proprioceptors play an important role in giving the sculpture information about its own state to enable model learning and adaptation.

Furthermore, Oudeyer \cite{Oudeyer2005} observed during the IAC experiments that the resulting robot behaviour had similarities to children's learning and playing behaviours. Their experiments showed that robot would focus on interacting with objects that have the next most complex responsive relationships to the its state and action. Since we expect human responses to be difficult to predict, after the robot has modelled its own mechanisms, the robot should then focus on generating behaviours that evoke reactions from the users. We hypothesize that this type of learning mechanism is well suited to interactive architecture systems, as the learning process itself will generate interesting behaviours and be an interesting feature of the installation to the visitors. 

We now adapt the IAC algorithm \cite{Oudeyer2007} to implement a learning architecture on the distributed architectural system described in Chapter \ref{chap:ctrl_system}. A distributed approach is necessary since the sculptures often contain hundreds of sensors and actuators distributed over an area the size of a large room. Each system node runs its own instantiation of the algorithm, called the CBLA Engine. A CBLA Engine is a thread that is iteratively executing the steps of the learning algorithm. It comprises a Node, an Action Selector, and an Expert, as illustrated in \Cref{fig:Block Diagram CBLA}. The following sections explain the functioning of each of these components.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1.0 \textwidth]{"fig/cbla/Block Diagram CBLA"}
	\caption[Block diagram of the Curiosity-Based Learning Algorithm]{(1) At the start of each time step, the Node outputs the current possible actions $M'(t)$s given $S(t)$. (2) Then, the Expert provides the action values $q$ associated with each possible $M'(t)$ to the Action Selector. (3) The Action Selector selects an action and actuates the Node. (4) After the Node has performed the action, it returns the actual resultant state $S(t+1)$ to the Expert. The Expert can then improve its internal prediction model and update the action value $q$ associated with the sensorimotor context $SM(t)$. }
	\label{fig:Block Diagram CBLA}
\end{figure}


\subsection{Node}
A node represents a subset of the sculptural system. Each node is specified by a set of sensor input and actuator output variables, and its own set of configuration parameters and functions, embodying the characteristics of the physical system that it represents. It sets the actuator outputs and samples the relevant sensory inputs.  The type and number of sensory inputs and actions are configurable; specific configurations used in our experiments will be described in more detail in \Cref{chap:validations}.

\subsection{Action Selector}

The action selector selects an action based on a list of possible actions, $M'(t)$, and their associated action values. It either selects an action that has the highest value (exploitation), or selects an action from the list at random (exploration). In our implementation, a version of the adaptive $\epsilon$-greedy \cite{Tokic2010} action selection method was used. $\epsilon$ specifies the probability that a random action is selected instead of the action with highest action value. The magnitude of $\epsilon$ changes linearly based on the magnitude of the action value, $q$, as shown in \eqref{eqn:exploring_rate}. 

\begin{subequations}\label{eqn:exploring_rate}
	\begin{equation}
		\epsilon = 
		\begin{cases}
			\epsilon_{max}  &\text{, if } q < q_{min} \\
			\epsilon_{min}  &\text{, if } q > q_{max} \\
			m \cdot q + b   &\text{, otherwise} 
		\end{cases}
	\end{equation}
	\begin{flalign}
		m &= \frac{\epsilon_{max} - \epsilon_{min}}{q_{min} - q_{max}} \\
		b &= \epsilon_{max} - m \cdot q_{min} 
	\end{flalign}
\end{subequations}
where $\epsilon$ is the exploration probability; $\epsilon_{max}$ and $\epsilon_{min}$ are constants representing the maximum and minimum exploration probability; $q$ is the action value; and $q_{min}$ and $q_{max}$ are constants representing the minimum and maximum action value thresholds.

When the action value, $q$,  is low, the probability of selecting a random action, $\epsilon$, is high since the current expected reward is low and it is worth spending time exploring new regions that may lead to higher rewards. On the other hand, when $q$ is high, $\epsilon$ is low, since the current expected reward is high and it is worth spending time exploiting a high reward region. Limits on maximum and minimum $\epsilon$ are set to ensure that the exploration probability is kept within a range.

\subsection{Expert}

An Expert consists of a prediction model, a set of exemplars, a Knowledge Gain Assessor (KGA), a region splitter, and potentially two child experts. 

An expert represents a region in the sensorimotor space defined by its exemplars. Each prediction is generated by the expert in the region with the most similar sensorimotor context. \Cref{fig:Block Diagram Expert} shows the internal mechanism of an Expert.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1.0 \textwidth]{"fig/cbla/Block Diagram Expert"}
	\caption[Block diagram of the Expert]{(1) The prediction model of the Expert first makes a prediction of the resultant sensor input $S'(t+1)$ based on the current sensorimotor context, $SM(t)$. (2) This prediction and the actual sensor input $S(t+1)$ are then fed into the KGA. $S(t+1)$ is also added to the collection of exemplars, which the prediction model is trained on at every time step. (3) After that, the KGA computes the reward and stores it in the Expert memory. A number of the most recent rewards are then recalled to evaluate a possible action given in the next time step that the Expert is active.}
	\label{fig:Block Diagram Expert}
\end{figure}


\subsubsection{Prediction Model}

The prediction model models the relationship between the node's input and output variables. At every time step, this model is used to make a prediction about the immediate future state based on the current state and the selected action. This prediction model is trained on the set of exemplars that were previously observed. These exemplars are represented as $[SM(t),S(t+1)]$ pairs. $SM(t)$ represents the sensorimotor context, i.e., the concatenation of the sensory state $S$ and the action $M$ taken at time $t$. $S(t+1)$ represents the observed sensory state at time $t+1$. 

In all the experiments presented in this thesis, linear models \eqref{eqn:linear-model} were used as the prediction models.
\begin{equation}\label{eqn:linear-model}
	S(t+t) = W \cdot SM(t)
\end{equation}
where W is a diagonal matrix.

Linear least squares regression or Lasso (Least Absolute Shrinkage and Selection Operator) \cite{Tibshirani1996} was used to obtain $W$. Nominally, linear regression is used due to its simplicity and the absence of parameters that require tuning. Lasso is used when the sensorimotor space includes input and output variables that are not proprioceptive. These variables tend to have low predictive power as they do not directly detect the changes in the outputs. Using Lasso can improve the model's predictability by driving the associated coefficients that are highly unpredictable non-predictive inputs to zero. Python library \textit{scikit-learn}'s\footnote{scikit-learn LinearRegression: \url{http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}},\footnote{scikit-learn Lasso: \url{http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html}} implementations of the two models were used. 

\subsubsection{Knowledge Gain Assessor}

The KGA calculates the reward given the predicted and actual sensory inputs. It is implemented as described in \cite{Oudeyer2007}. \Cref{fig:Block Diagram KGA} illustrates the internal workings of the KGA. The metaM returns the predicted mean error, $<e(t+1-\tau)>$, which in this case is simply the average error over a window of previous errors with an offset, $\tau$. The reward is calculated by subtracting it by the actual mean error, $<e(t+1)>$. This reward is then used by the action selector to compute the action values. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=1.0 \textwidth]{"fig/cbla/Block Diagram KGA"}
	\caption[Block diagram of the Knowledge Gain Assessor]{$S(t+1)$ and $S'(t+1)$ are the actual and predicted sensor input variables. (1) The KGA computes the error by taking their root-mean-square error difference. (2) After that, it computes the mean error over a window of previously calculated errors. Note that the mean error is only calculated based on errors associated with this particular region. (3) The metaM predicts the error also by taking the mean error a window of previously calculated errors with an offset of $\tau$. (4) Finally, the KGA outputs the reward by taking the difference between the actual mean error and predicted mean error. }
	\label{fig:Block Diagram KGA}
\end{figure}

\FloatBarrier
\subsubsection{Region Splitter}

The system initially has only one region. All new exemplars are added to the same region. As more exemplars are added to memory, once split criteria 1 and 2 are met, a region will split into two parts.

Criterion 1: 
\begin{equation}\label{eqn:split-criterion1}
	N>\tau_1
\end{equation}
N is the number of exemplars; and $\tau_1$ is the threshold that is inherited from the expert's parent. 

Criterion 2:	
\begin{equation}\label{eqn:split-criterion2}
	 e>\tau_2
\end{equation}	
$e$ is the mean error of the region and $\tau_2$ is a threshold parameter. 

Criterion 1 specifies that a region can only split into two when it contains a sufficient number of exemplars, to ensure that there will be enough training data in the sub-regions. Criterion 2 specifies that a region will only split if prediction error is high. This prevents learnt models with high accuracy from being split indefinitely. 
If the split criteria are met, the Region Splitter finds a cut value and a cut dimension that minimizes the average variance in each sub-region. 

After the best cut value and cut dimension are identified, the exemplars of the parent region are split between the two regions based on the cut dimensions and values. The high-level mechanism of the region splitting process is illustrated in \Cref{fig:Block Diagram RegionSplitter}.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1.0 \textwidth]{"fig/cbla/Block Diagram RegionSplitter"}
	\caption[Block diagram of the Region Splitter]{When the function “split()” is called, it first checks if the split criteria are met by calling ``is\_splitting()''. If the split criteria are met, it forwards the exemplars to the Region Splitter. The Region Splitter then splits the exemplars into two and assigns them to the Left and Right Expert. Other properties such as the previous errors and rewards are passed on as well. }
	\label{fig:Block Diagram RegionSplitter}
\end{figure}

\FloatBarrier
\subsection{Pseudocode of CBLA}
	
The overall process is described in the pseudocode for the CBLA and is shown as Algorithm~\ref{alg:pseudocode-cbla}.

\begin{algorithm}[!htb]
	\caption{Pseudocode for the CBLA} 
	\label{alg:pseudocode-cbla}
	\begin{algorithmic} [1]
		\State $t\gets 0$
		\State $S(t)\gets S(0)$
		\Loop
			\State $[Possible\_M(t)] \gets Node.get\_possible\_action(S(t)) $
			\State $[Action\_Value(M(t))] \gets Expert.evaluate\_action(Possible\_M(t))$
			\State $M(t) \gets Action\_Selector.select\_action([Possible\_M(t), Action\_Value]) $
			\State $Node.actuate(M(t)) $
			\State $S(t+1) \gets Node.report()$
			\State $S'(t+1) \gets Expert.predict(S(t), M(t))$
			\State $Expert.append(S(t), M(t), S(t+1), S'(t+1))$
			\State $Expert.split()$
			\State $t \gets t + 1 $
		\EndLoop
	\end{algorithmic}
\end{algorithm}


\subsection{Differences between the CBLA Engine and the IAC algorithm}

In developing the CBLA Engine described above, several adaptations have been made to the IAC algorithm to enable application to the kinetic sculpture installation. 

\subsubsection{Region Splitter}

Several improvements are made to the logic of the Region Splitter. As we run the algorithm over an extended period of time, the expert might have already learnt the model of the system as best as possible given the available sensory information. Since Oudeyer's method simply splits a region when the number of exemplars exceeds a certain threshold, it will continue to split regions even when the expert associated with that region has already acquired a good model with low prediction error. This is undesirable as it leads to over-fitting and makes the system harder to adapt when the system itself or the environment changes. 

In our implementation, an additional prediction error threshold is added to prevent regions from splitting when the prediction error is low. Moreover, there is no reason to split the region if the split does not improve the prediction accuracy. Therefore, after the split, the split quality must also be above a threshold. If it is not, the split will be retracted. This split quality is measured by the magnitude of the average within-group variance relative to the overall variance.

During the early stages of the learning process, learning opportunities are plenty. Setting the split number threshold too high can hamper the speed of learning. However, over time, easy learning opportunities diminish and complex regions require a larger number of exemplars. Thus, the split number threshold grows at a constant rate every time a split happens. This way, regions that are less explored can maintain low thresholds which promote learning, while mature regions have higher thresholds which promote gathering of more exemplars and reduce over-fitting.

In addition, in \cite{Oudeyer2007}, the cut value and cut dimension are determined by generating a set of random cut values for each dimension and computing the corresponding variances of the resultant sensor inputs. The cut value and dimension pair with the lowest total variance is selected. The total variance is the sum of the variances of $S(t+1)$ in each dimension. In our implementation, changes are made to the way regions splitting is done.

First, in order to take the interdependency among dimensions into account, instead of using the $S(t+1)$ value directly when computing variances, herein, its principal components are used. Using its principal components can better reflect the underlying structure of the data by emphasizing the differences among data points along the direction of maximum variance. Note that we do not compute the principal components of the $SM(t)$, where the cut value and cut dimension are specified. Instead, the evaluation is based on the principal components of $S(t+1)$ for which we want to minimize the variance. 

Second, in \cite{Oudeyer2007}, if one dimension tends to have a lower variance than another, that dimension is much more likely to get selected, even if the cut does not lower its resultant variance. Since region splitting aims to separate groups into low variance regions, this is unproductive as it does not decrease the variance. Here, instead of using the variance, relative variance is used. It is computed by taking mean of the variance of the region divided by the total variance of the overall region before splitting. This gives a better indicator on whether or not the split results in generating lower variance regions. A relative variance of 1 implies that the split does not improve the variance. On the other hand, a low relative variance implies that the split decreases the regional variance. This method can reduce the number of unproductive splits that fragment the state space.

\begin{equation}
S_{relative} = \frac{S_1+S_2}{S_{1,2}}
\end{equation}
$S_1$ and $S_2$ are the total variances of the two regions split using the candidate cut value and cut dimension; $S_{1,2}$ is the total variance of the two combined regions.

Furthermore, Oudeyer's method is susceptible to leaving a small number of exemplars near the ideal cut value incorrectly split. \Cref{fig:bad-cut-example} shows an example that illustrates this problem. The blue squares represent the exemplars in a region. In this example, the green dotted line represents the ideal cut value as it would minimize the average within-group variance of the two groups after the split. If potential cut values are selected randomly as done in \cite{Oudeyer2005}, cut values like those shown as red lines might be selected. In this case, cut value (b) would be chosen, as it would result in the lower average variance than line (a) and (c). This would place the first point on the right side of the line (b) in the wrong group. Next time the regions need be split, another attempt would be made to make a split near the ideal cut value (green dotted line). This would result in an excessive number of similar regions near the ideal cut value. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7 \textwidth]{"fig/cbla/bad-cut-example"}
	\caption[An example of making undesirable cuts using randomly selection method]{An example illustrating the problem with selecting potential cut values at random. The x-axis represents one dimension in the sensorimotor context, which is where the cut values are chosen from. The y-axis represents one dimension of the resultant state, where the variances are computed. Each blue square represents an exemplar in a region that is being split. The green dotted line represents the ideal cut value. The red lines represent a set of potential cut values selected randomly.}
	\label{fig:bad-cut-example}
\end{figure}

On the other hand, although one can simply iterate through every exemplar in the region and find the cut value with the lowest variance, this is impractical, as a significant number of iterations are required as the number of dimensions and exemplars increase. 

To improve the likelihood of finding the best cut value while keeping the number of iterations low, a new divide-and-zoom-in method is implemented to focus on promising ranges of cut values. The toy example in \Cref{fig:divide-and-zoom-in} illustrates how this method works. In a region with $N$ exemplars, there are $N-1$ possible cut values for each dimension. Each dot represents the actual relative variances if the region is split in between each pair of exemplars. However, the relative variances are not evaluated for all of them. In \Cref{fig:divide-and-zoom-in_1}, all possible values are identified by the algorithm and plotted along the x-axis. The y-axis shows their true relative variances, though they are not known by the algorithm. The colour red indicates that the relative variance for the cut value has not been evaluated. 

Then, the relative variances of the smallest, largest, and the $k$ potential cut values evenly spaced in the middle are evaluated as shown in \Cref{fig:divide-and-zoom-in_2} ($k=5$). The colour dark blue indicates that the relative variance for the cut value has been evaluated. After that, the point with the lowest relative variance as indicated by the yellow ring and its two adjacent evaluated points are identified. In the next iteration, the algorithm zooms in to the range between those two adjacent points as indicated by the light blue region. Note that the value of $k$ must be greater than 3, and if $k$ is set to $N-1$, it is equivalent to evaluating every possible value. 

The aforementioned process is then repeated in the new zoomed-in region as shown in \Cref{fig:divide-and-zoom-in_3}. This continues until the number of potential cut values in the zoomed-in range is fewer than or equal to $k$. When that happens, in the final step shown in \Cref{fig:divide-and-zoom-in_3}, the relative variance for potential cut value within the range at that iteration is evaluated. The one with the lowest relative variance is selected as the best cut value for that particular cut dimension. 
This entire process is repeated for each dimension and the cut dimension and cut value pair with the lowest relative variance is selected for the splitting process.


\begin{figure}[!htbp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=1.0 \textwidth]{"fig/cbla/divide-and-zoom-in_1"}
		\caption{Each dot represents the actual relative variances if the region is cut in between each pair of exemplars in a region. At this point, the relative variances have not been calculated yet and this is indicated by the colour red.}
		\label{fig:divide-and-zoom-in_1}
	\end{subfigure}	
	~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=1.0 \textwidth]{"fig/cbla/divide-and-zoom-in_2"}
		\caption{In the first iteration, the relative variance of a subset of all possible cut values (shown as dark blue dots) are evaluated. The one with the lowest relative variance (yellow ring) and its two adjacent evaluated points are identified. This process is then repeated in the range between those two points (light blue) in the next iteration.}
		\label{fig:divide-and-zoom-in_2}
	\end{subfigure}	
	
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=1.0 \textwidth]{"fig/cbla/divide-and-zoom-in_3"}
		\caption{In the second iteration, the computation is repeated in a smaller, zoomed-in range. It continues until the number of potential cut values in the zoomed-in range is below a threshold.}
		\label{fig:divide-and-zoom-in_3}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=1.0 \textwidth]{"fig/cbla/divide-and-zoom-in_4"}
		\caption{In the final step, the relative variance for every potential cut value within the range is evaluated. The one with the lowest relative variance is selected as the best cut value.}
		\label{fig:divide-and-zoom-in_4}
	\end{subfigure}
	\caption[Illustrations explaining the ``Divide-and-Zoom-In'' region split values computation method]{A toy example explaining the ``Divide-and-Zoom-In'' region split values computation method.}
	\label{fig:divide-and-zoom-in}
\end{figure}

This method can at least find a local minimum that cleanly splits a region into two because it searches for cut value with the lowest variance by checking every possible cut value within a range bounded by higher variance values. It keeps the number of relative variance computations low by focusing on promising regions where cut values with relatively low variances can be found. Though this method may not necessarily find the global minimum, it is better than simply computing the variances of a set of random cut values given the same number of evaluations. It avoids the problem of points near a local minimum being split into multiple regions over successive splitting processes. 

\FloatBarrier 
\subsubsection{Action Selection}\label{sec:action_selection}

Visually, it is difficult to distinguish between the periods when the IAC is learning from the periods when the IAC is simply executing random actions after all regions are learnt. In order to better visualize the learning process, we introduce some constraints that produce different actions given different levels of activation. 

We considered two different types of implementations. In the first implementation, we introduced Idle mode. The Idle Mode action is chosen as the action that requires the least power. This gives the visitors the sense that the sculpture is resting. During Idle Mode, the system selects the idle action a majority of the time. Otherwise, it will select an action based on its regular action selection policy. The CBLA engine enters Idle Mode when there is small knowledge gain potential. Conversely, it exits Idle Mode hen the knowledge gain potential, or the change of it is higher than a threshold. Once it exits Idle Mode, it must stay out of Idle Mode for at least a certain period of time before being allowed to enter Idle Mode again. 

Alternatively, instead of having a hard threshold that moves the system from totally quiet to totally active, we used a sigmoid function shown in \Cref{eqn:map_sigmoid} to define a range of activations from low activation to high activation. In this formulation, we capped the maximum output level $m_{max}$ based on the knowledge gain potential $x$. This way, there is a visually lower level of activation while the knowledge gain potential is low. 

\begin{subequations}\label{eqn:map_sigmoid}
	\begin{flalign} 
		m_{max} &= \frac{1}{1+e^{-k+a}}; \quad k > 0 \\
		a &= \log{(\frac{1}{b}-1)}; \quad 0 < b < 1
	\end{flalign}
\end{subequations}
$m_{max}$ is the maximum output level which can be a number between 0 to 1; $x$ is the knowledge gain potential; $b$ determines the minimum output level when knowledge gain potential is 0; $k$ determines the steepness of the sigmoid function. 

To compute the knowledge gain potential, we either use average action value or relative action value. Average action value requires calibration and is not adaptive to fundamental changes in the system like when a sensor is covered or disconnected. 

Relative action value is used to reduce the difficulty of tuning for different actuators as different actuators have different inherent level of action values range. This relative action value is calculated by dividing the squared action value of this time step by average squared action values over the previous $W$ time steps as shown in \Cref{eqn:action_val_set,eqn:avg_action_val,eqn:relative_action_val}. This means that even if there is a fundamental change to the system, it can still adapt over time.  

\begin{equation}\label{eqn:action_val_set}
	Q_z = [q_{z-W}, q_{z-W-1}, ... , q_{z-1}]
\end{equation}
$Q_z$ is the set of previous $W$ action values $q$ at time step $z$; $W$ is the window size and it is a constant.
\begin{equation}\label{eqn:avg_action_val}
	\overline{q^2_z} = \frac{1}{W}\displaystyle\sum_{i=z-W}^{z-1} q_i^2
\end{equation}
$\overline{q^2_z}$ is the average squared action value at time step $z$ over the window of width $W$.
\begin{equation}\label{eqn:relative_action_val}
	\widehat{q^2_z} = \frac{q_t^2}{max(\overline{q^2_z}, \nu)}
\end{equation}
$\widehat{q^2_z}$ is the relative action value; $q_z^2$ is the current action value; $\nu$ is the sensitivity constant.

One consequence of this formulation is that the denominator will become too small over time and a very small increase in action value can trigger large activation. This can be a desirable feature as it creates a system that periodically wakes up in hope to attract visitors when there is nothing to be learnt for long time. This sensitivity level during low learning period can be adjusted by changing the value of $\nu$. On the other hand the window size $W$ affects how quickly the system can adapt to a new ambient environment. 


\section{Multi-Node CBLA System}\label{sec:cbla-system}

One option for controlling a larger system of sensors and actuators is to control them centrally through a single CBLA engine.  However, if all output and input variables are grouped into one node, the sensorimotor state will become very large and it might take a very long time for the system to converge and react to changes. Therefore, subsets of related variables are grouped into nodes. Each node runs on its own CBLA engine in parallel.  While our implementation uses a centralized high-level processor to compute these parallel CBLA engines, this architecture also allows for distribution of the software agents to local processors.

\subsection{Sensorimotor Context of a CBLA Node}

There are several options for grouping sensors and actuators into nodes.  One approach is to group actuators and their associated proprioceptive sensors by function, because these relationships are the easiest to model. In addition, if multiple actuators work together to perform one function that directly affects one sensor, they should be grouped as a node because their relationship must be considered in order produce a prediction model for the sensor input. However, grouping simply by function cannot capture environmental changes and occupants' interaction effectively since nodes with the same functions are typically distributed over the entire sculptural system.

Another approach is to group sensor input and actuator output variables by spatial proximity. Since environmental changes and occupant interaction are likely to affect components that are close together physically, this will allow the system to capture those dynamics more effectively. 

However, all sensor input and actuator output variables associated with a particular node are updated once per loop. This means that the loop speed is limited by the slowest actuator or sensor within the node. For instance, an Fin node that is actuated by shape memory alloy (SMA) wire\footnote{Dynalloy Flexinol Actuator Wire:  \url{www.dynalloy.com/tech_data_wire.php}} has a heating and cooling cycle time of about 12 seconds, while an LED can be turned on and off many degrees of magnitude faster. This is very inefficient and limiting for the fast components. Therefore, components running at different speeds should be grouped into different nodes. This allows each node to run at its own speed, independently from the other nodes. 

In our implementation, each node is constructed of components that are related functionally and proximally. For instance, LED output and ambient light sensor are in one node; Fin motion and accelerometer are in one node. Different nodes run at different frequencies and capture the dynamics of the system at different time scales. We limited the number of outputs per node to one. This structure keeps the system's dimensionality low and flexible. This also allows us to optimize the CBLA loop rate for each type of actuator as the bandwidth is mainly constrained by the bandwidth imposed by the actuators. Also, no more than one node can control one physical actuator since this will result in conflicts. 

\subsection{Inter-Node Connections}\label{sec:inter-node connections}

Nevertheless, if the nodes perform learning independently, the CBLA does not actually capture the dynamics of the entire system. To integrate the nodes into one coherent entity, we have used shared sensors and virtual inputs.

By sharing input variables that share the same sensor, system changes effected by one node can be detected by another node indirectly. For instance, two adjacent nodes might share a single IR Proximity sensor. This way, change in the environment can be seen by both nodes. However, when two Nodes share one input variable, if the action of the actuator does not affect the shared sensor, information about that event is not transferred to the other Node. 

Another method is to treat output signal from a node as input to other nodes. We called this a virtual input. This way environmental changes happening to one node can be detected by other nodes indirectly in the time steps afterwards. The idea is that external disturbances, like a user standing in front of the sensor of a node, may be represented as an output signal originating from that node, which may be measured by another node. This gives the other node information about the fact that there is someone, albeit being somewhat delayed. \Cref{fig:Indirect input diagram} illustrates how Node 2 can detect changes in the environment that are not covered by its own sensors through Node 1. The reason for doing this is that the changes in the environment that are only detectable by Node 1 might be an effect of an action selected by Node 2. This mechanism allows Node 2 to capture these kinds of complex relationships.


\begin{figure}[!htb]
	\centering
	\includegraphics[height=0.20 \textheight]{"fig/cbla/Indirect input diagram"}
	\caption[Illustration explaining how virtual inputs may be used to detect changes outside of a Node's sensorimotor context]{An example of illustrating how Node 2 can indirectly detect the presence of a user through Node 1. The presence of the user may affect the output of Node 1. Since Node 2 takes the output of Node 1 as input, Node 2 would in effect respond to the presence of the user indirectly despite not having a sensor that can detect the user in its sensorimotor context.}
	\label{fig:Indirect input diagram}
\end{figure}

\subsection{Summary}

In this chapter, the implementation of the CBLA Engine, a reinforcement learning algorithm that selects actions that lead to the maximum potential knowledge gain, is detailed. Each engine represents a subset of the system and constructs a model that predicts the consequences of its actions given its current states. A system is then formed by connecting a network of CBLA Engines through shared sensors and virtual inputs. The distributed approach enables the system to respond to the occupants and adapt to localized changes in the surroundings quickly. Building on this CBLA system, a sculptural system that generates its own life-like, interactive behaviours may emerge.

\FloatBarrier  

