\chapter[Curiosity-Based Learning Algorithm]
{Curiosity-Based Learning Algorithm\footnote{An early version of this chapter has been submitted to IROS 2015 \cite{Chan2015} }} 
\label{chap:cbla}
 
In this section, we first briefly describe the Intrinsic Adaptive Curiosity (IAC) algorithm proposed by Oudeyer \cite{Oudeyer2007}. Then, our approach for adapting and generalizing the algorithm for implementation on a distributed kinetic sculpture is presented. This includes additional parameters, the introduction of an idle mode, and the integration of multiple independent learning agents in a distributed network. 

\section{Intrinsic Adaptive Curiosity}

In \cite{Oudeyer2007}, Oudeyer's goal was to develop a robot that is capable of life-long learning. The robot makes sense of the mapping between its actuators and sensors through continuous self-experimentation, without explicit instruction from human experts. Fundamentally, the IAC is a reinforcement learning algorithm with an objective to maximize learning progress. Learning progress is defined as the reduction in prediction error. In other words, the agent seeks to explore the region of the sensorimotor space that leads to the highest improvement in predication error. As a result, the algorithm avoids learning in the parts of sensorimotor space that are either too random or too predictable. This formulation leads to continual change in behaviour over time, as regions of the state space that are initially interesting become less interesting as the system becomes more knowledgeable about them. 
 
The system consists of two learning mechanisms, the Classic Machine learner (classM) and the Meta Machine learner (metaM).  Based on the context (the sensors' inputs) and the action (the actuators' outputs), classM computes a prediction of the consequence, i.e., the resultant sensors' inputs at the next time step. Then, it compares the actual consequence with the prediction and modifies its model in order to reduce the error in the subsequent prediction. The Meta Machine learner (metaM) predicts the error of classM. In other words, it estimates how accurately classM is able predict the consequence. The actual prediction error is then fed back to the metaM, and metaM modifies its estimate of prediction error of the newly modified classM. This change in the prediction error is recorded at each time step in the knowledge gain assessor (KGA). The KGA then computes the expected learning progress by calculating the change in error estimation.  This expected learning progress is used as the reward. 

In Oudeyer's paper \cite{Oudeyer2007}, one important feature is the idea of regional experts. Each region collects exemplars of similar sensorimotor context, and has an expert that is trained on the exemplars in the region. Exemplars are the training data for the prediction model and they are collected as the system selects actions and observes the consequences. The "features" are the sensory inputs $S(t)$, and selected actions $M(t)$ at time $t$; and the "labels" are the resultant sensory inputs $S(t+1)$ at time $t+1$. The regional experts constrain the estimate of learning progress within their respective sensorimotor contexts. This is important because it allows each expert to use a simpler model, as it covers only a small region of the state space.  

The following are the steps for the learning algorithm:

\begin{enumerate}
	\item \label{learning_start} Read sensory inputs $S(t)$
	\item Select action $M(t)$ with the highest predicted learning progress $R(SM(t))$ based on $\epsilon$-greedy selection policy
	\item Consult expert specialized in the relevant sensorimotor context SM(t) to predict the expected sensory inputs $S(t+1)'$
	\item Perform action $M(t)$
	\item Observe actual sensory inputs $S(t+1)$ and add to the expert's training set
	\item Compute prediction error $e(t+1) = S(t+1) – S(t+1)'$
	\item Compute the change in prediction error $R(SM(t)) = -[e(t+1) - e(t)]$
	\item Update the learning progress for the sensorimotor context $R(SM(t))$
	\item \label{learning_loop} Repeat \ref{learning_start} to \ref{learning_loop} indefinitely
\end{enumerate}
	

\section{CBLA Engine}

Oudeyer \cite{Oudeyer2005} observed during the IAC experiments that the resulting robot behaviour had similarities to children's learning and playing behaviours. We hypothesize that this type of learning mechanism is well suited to interactive architecture systems, as the learning process itself will generate interesting behaviours and be an interesting feature of the installation to the visitors. 

We now adapt the IAC algorithm \cite{Oudeyer2007} to implement a learning architecture on the distributed architectural system described in Chapter \ref{chap:ctrl_system}. Each system node runs its own version of the algorithm, called the CBLA Engine. A CBLA Engine is a thread that is iteratively executing the steps of the learning algorithm. It is comprised of a node, an action selector, and an expert, as illustrated in \Cref{fig:Block Diagram CBLA} 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0 \textwidth]{"fig/cbla/Block Diagram CBLA"}
	\caption[Block diagram of the Curiosity Based Learning Algorithm]{(1) At the start of each time step, the Node outputs the current possible actions $M(t)$'s given $S(t)$. (2) Then, the Expert provides the Action Values associated with those possible $M(t)$ to the Action Selector. (3) After that, the Action Selector selects an action and actuates the Node. (4) After the Node has performed the action, it returns the actual resultant state $S(t+1)$ to the Expert. The Expert can then improve its internal prediction model and update the Action Value associated with $SM(t)$. }
	\label{fig:Block Diagram CBLA}
\end{figure}


\FloatBarrier
\subsection{Node}
A node represents a subset of the sculptural system. Each node is specified by set of sensor input and actuator output variables, and its own set of configuration parameters and functions, embodying the characteristics of the physical system that it represents. It communicates with the lower-level layer to set the actuator outputs and sample the relevant sensory inputs.  The type and number of sensory inputs and actions are configurable, specific configurations used in our experiments will be described in more detail in \Cref{chap:validations}.

\subsection{Action Selector}

The action selector selects an action based on a list of possible actions, $M(t)$, and their associated action values. It either selects an action that has the highest value (exploitation), or selects an action from the list at random (exploration). In our implementation, a version of the adaptive $\epsilon$-greedy \cite{Tokic2010} action selection method was used. $epsilon$ specifies the probability that a random action is selected instead of the action with highest action value. The magnitude of $epsilon$ changes based on the magnitude of the action value. When the action value is low, the probability of selecting a random action is high since the current expected reward is low and it is worth spending time exploring new regions that may lead to higher rewards.

\subsection{Expert}

An Expert consists of a prediction model, a set of exemplars, a Knowledge Gain Assessor (KGA), a region splitter, and potentially two child experts. 

An expert represents a region in the sensorimotor space defined by its exemplars. Each prediction is generated by the expert in the region with the most similar sensorimotor context. \Cref{fig:Block Diagram Expert} shows the internal mechanism of an Expert.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1.0 \textwidth]{"fig/cbla/Block Diagram Expert"}
	\caption[Block diagram of the Expert]{(1) The prediction model of the Expert first makes a prediction of the resultant sensor input $S(t+1)'$ based on the current sensorimotor context, $SM(t)$. (2) This prediction and the actual sensor input $S(t+1)$ is then fed into the KGA. $S(t+1)$ is also added to the collection of exemplars, which the prediction model is trained on at every time step. (3) After that, the KGA computes the reward and updates the Action Value associated with $SM(t)$ in the Expert memory. These Action Values are recalled to evaluate a possible action in the next time step the Expert is active.}
	\label{fig:Block Diagram Expert}
\end{figure}

\subsubsection{Prediction Model}

The prediction model models the relationship between the node's input and output variables. In all the experiments presented in this paper, linear regression or Lasso was used. At every time step, the model is used to make a prediction about the immediate future state based on the current state and the selected action. This prediction model is trained on the set of exemplars that were previously observed. These exemplars are represented as $[SM(t),S(t+1)]$ pairs. $SM(t)$ represents the sensorimotor context, i.e., the concatenation of the sensory state and the action taken at time $t$. $S(t+1)$ represents the observed sensory state at time $t+1$. 

\subsubsection{Knowledge Gain Assessor}

The KGA calculates the reward given the predicted and actual sensory inputs. \Cref{fig:Block Diagram KGA} illustrates the internal workings of the KGA. The metaM in this case simply returns the average error over a window of time $\tau$ time steps ago, $<e(t+1)-\tau>$. The reward is calculated by subtracting the current mean error, $<e(t+1)>$. This reward is then used by the action selector for computing the action values. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0 \textwidth]{"fig/cbla/Block Diagram KGA"}
	\caption[Block diagram of the Knowledge Gain Assessor]{$S(t+1)$ and $S'(t+1)$ are the actual and predicted sensor input variables. (1) The KGA computes the error by taking their root-mean-square error difference. (2) After that, it computes the mean error over a window of previously calculated errors. Note that the mean error is only calculated based on errors associated with this particular region. (3) The metaM predicts the error by taking the mean error over time. (4) Finally, the KGA outputs the Reward by taking the difference between the actual mean error and predicted mean error. }
	\label{fig:Block Diagram KGA}
\end{figure}

\FloatBarrier
\subsubsection{Region Splitter}

The system initially has only one region. All new exemplars are added to the same region. As more exemplars are added to memory, once split criteria 1 and 2 are met, a region will split into two parts.

Criterion 1: 
\begin{equation}\label{eqn:split-criterion1}
	N>\tau_1
\end{equation}
N is the number of exemplars; and $\tau_1$ is the threshold that is inherited from the expert's parent. 

Criterion 2:	
\begin{equation}\label{eqn:split-criterion2}
	 e>\tau_2
\end{equation}	
$e$ is the mean error of the region and $\tau_2$ is a threshold parameter. 

Criterion 1 specifies that a region can only split into two when it contains a sufficient number of exemplars, to ensure that there will be enough training data in the sub-regions. Criterion 2 specifies that a region will only split if prediction error is high. This prevents learnt models with high accuracy from being split indefinitely. 
If the split criteria are met, the Region Splitter finds a cut value and a cut dimension that minimizes the average variance in each sub-region. 


After the best cut value and cut dimension is identified, the exemplars of the parent region are split between the two regions based on the cut dimensions and values. The high-level mechanism of the region splitting process is illustrated in \Cref{fig:Block Diagram RegionSplitter}.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1.0 \textwidth]{"fig/cbla/Block Diagram RegionSplitter"}
	\caption[Block diagram of the Region Splitter]{When the function “split()” is called, it first checks if the split criteria are met by calling "is\_splitting()". If the split criteria are met, it forwards the exemplars to the Region Splitter. The Region Splitter then splits the exemplars into two and assigns them to the Left and Right Expert. Other properties such as the previous errors and rewards are passed on as well. }
	\label{fig:Block Diagram RegionSplitter}
\end{figure}

\subsection{Pseudocode of CBLA}
	
The overall process is described in the pseudocode for the CBLA and is shown as Algorithm~\ref{alg:pseudocode-cbla}.

\begin{algorithm}[H]
	\caption{Pseudocode for the CBLA} 
	\label{alg:pseudocode-cbla}
	\begin{algorithmic} [1]
		\State $t\gets 0$
		\State $S(t)\gets S(0)$
		\Loop
			\State $[Possible\_M(t)] \gets Node.get\_possible\_action(S(t)) $
			\State $[Action\_Value(M(t))] \gets Expert.evaluate\_action(Possible\_M(t))$
			\State $M(t) \gets Action\_Selector.select\_action([Possible\_M(t), Action\_Value]) $
			\State $Node.actuate(M(t)) $
			\State $S(t+1) \gets Node.report()$
			\State $S'(t+1) \gets Expert.predict(S(t), M(t))$
			\State $Expert.append(S(t), M(t), S(t+1), S'(t+1))$
			\State $Expert.split()$
			\State $t \gets t + 1 $
		\EndLoop
	\end{algorithmic}
\end{algorithm}


\subsection{Differences between the CBLA Engine and the IAC algorithm}

In developing the CBLA Engine described above, several adaptations have been made to the IAC algorithm to enable application to the kinetic sculpture installation. 

\subsubsection{Region Splitter}

Several improvements are made to the logic of the Region Splitter. As we run the algorithm over an extended period of time, the expert might have already learnt the model of the system as best as possible given the available sensory information. Since Oudeyer's method simply splits a region when the number of exemplars exceeds a certain threshold, it will continue to split regions even when the expert associated with that region has already acquired a good model with low prediction error. This is undesirable as it leads to over-fitting and makes the system harder to adapt when the system itself or the environment changes. 

In our implementation, an additional prediction error threshold is added to prevent regions from splitting when the prediction error is low. Moreover, if the split does not improve the prediction accuracy, there is no reason to split the region. After the split, the split quality must also be above a threshold. If it is not, the split will be retracted. This split quality is measured by the magnitude of the average within-group variance relative to the overall variance.

During the early stages of the learning process, learning opportunities are plenty. Setting the split number threshold too high can hamper the speed of learning. However, over time, easy learning opportunities diminish and complex regions require a larger number of exemplars. Thus, the split number threshold grows at a constant rate every time a split happens. This way, regions that are less explored can maintain low thresholds which promote learning, while mature regions have higher thresholds which promote gathering of more exemplars and reduce over-fitting.

In addition, in \cite{Oudeyer2007}, the cut value and cut dimension are determined by generating a set of random cut values for each dimension and computing the corresponding variances of the resultant sensor inputs. The cut value and dimension pair with the lowest total variance is selected. Total variance is sum of the variances of $S(t+1)$ in each dimension. However, this method has several problems. 

First, this method does not take into account the interdependency among dimensions. Instead of directly using the $S(t+1)$ value directly when computing variances, its principal components are used. Note that we are not taking the principal component of the $SM(t)$ which is where we specify the cut value and cut dimension. Instead, it only applies to $S(t+1)$ which we want to minimize its variance. 

Second, if one dimension tends to have a lower variance than another, such dimension is much more likely to get picked, even if the cut does not lower its resultant variance. Since region splitting aims to separate groups into low variance regions, this is unproductive as it does not decrease the variance. Instead of using the variance, relative variance is used. It is computed by taking mean of the variance of the region divided by the total variance of the overall region before splitting. This gives a better indicator on whether or not the split results in generating more concentrated regions. A relative variance of 1 implies that the split does not improve and should not be cut in this way. On the other hand, a low relative variance implies that the split decreases the regional variances. This method can reduce the amount of unproductive splits that fragment the state space.

\begin{equation}
S_{relative} = \frac{S_1+S_2}{S_{1,2}}
\end{equation}
$S_1$ and $S_2$ are the total variances of the two regions split using the candidate cut value and cut dimension; $S_{1,2}$ is the total variance of the two combined regions.

Furthermore, this method is susceptible to leaving a small number of sample points near a desirable cut value since the random selection might not pick up the exemplars that are right at the edge. On the other hand, although one can simply iterate through every exemplar in the region and find the cut value with the lowest variance, this is impractical. This implies that a significant number of iterations is required as the number of dimensions and exemplars increase. To improve the likelihood of finding the best cut value while keeping the number of iterations low, a new divide-and-zoom-in method is implemented to focus on promising ranges of cut values. The toy example in  \Cref{fig:divide-and-zoom-in_1,fig:divide-and-zoom-in_2,fig:divide-and-zoom-in_3,fig:divide-and-zoom-in_4} illustrates how this method works. This method is more likely to find the local minimum which cleanly splits a region into two while keeping the number of relative variance computation low. 

\begin{figure}[htbp]
	\centering
	\includegraphics[height=0.25 \textheight]{"fig/cbla/divide-and-zoom-in_1"}
	\caption[Illustration explaining the "Divid-and-Zoom-In" region split values computation method 1]{An toy example of how relative variance changes with different cut values. At this point, the relative variances have not been calculated yet. The red dots represent the actual relative variances before they are calculated by the algorithm.}
	\label{fig:divide-and-zoom-in_1}
\end{figure}	
\begin{figure}[htbp]
	\centering
	\includegraphics[height=0.3 \textheight]{"fig/cbla/divide-and-zoom-in_2"}
	\caption[Illustration explaining the "Divid-and-Zoom-In" region split values computation method 2]{This is the first iteration of the toy example. First, compute the relative variances of $N/k$ of the cut values (blue); then, select the point with the lowest relative variance (yellow ring); after that, identify the two adjacent points and zoom in to the range between those two points in the next iteration (light blue)}
	\label{fig:divide-and-zoom-in_2}
\end{figure}	
\begin{figure}[htbp]
	\centering
	\includegraphics[height=0.25 \textheight]{"fig/cbla/divide-and-zoom-in_3"}
	\caption[Illustration explaining the "Divid-and-Zoom-In" region split values computation method 3]{This is the second iteration of the toy example. The computation is repeated in a smaller, zoomed-in range.}
	\label{fig:divide-and-zoom-in_3}
\end{figure}	
\begin{figure}[htbp]
	\centering
	\includegraphics[height=0.25 \textheight]{"fig/cbla/divide-and-zoom-in_4"}
	\caption[Illustration explaining the "Divid-and-Zoom-In" region split values computation method 4]{Zooming in until the variance for every point within the range is computed.}
	\label{fig:divide-and-zoom-in_4}
\end{figure}


\FloatBarrier 
\subsubsection{Action Selection}

Visually, it is difficult to distinguish between the periods when the CBLA is learning from the periods when the CBLA is simply executing random actions after all regions are learnt. In order to better demonstrate the learning process, we introduce some constrains that produce different action given different level of activation. 

We considered two different types of implementations. First is introducing idle Mode. The idle mode action is chosen as the action that requires the least power. This gives the visitors the sense that the sculpture is resting. During idle mode, the system selects the idle action a majority of the time. Otherwise, it will select an action based on its regular action selection policy. The CBLA engine enters idle mode when there is large knowledge gain potential. Conversely, it exits idle mode when the knowledge gain potential, or the change of it is higher than a threshold. Once it exits idle mode, it must stay in non-idle for at least a certain period of time before being allowed to enter idle mode again. 

Alternatively, instead of having a hard threshold that go from totally quiet to totally active, we used a sigmoid function shown in Equation~\eqref{eqn:map_sigmoid} to define the region of low activation to high activation. In this formulation, we capped the maximum output level $m_{max}$ based on the knowledge gain potential $x$. This way, there is a visually lower level of activation while the knowledge gain potential is low. In an exhibition perspective, it is better to always have some kinds of activation to entice people than having a long dead period which user might be gone before the sculpture gets activated.

\begin{subequations}\label{eqn:map_sigmoid}
	\begin{flalign} 
		m_{max} &= \frac{1}{1+e^{-k+a}}  \\
		k &= -\frac{1}{c}[\log{(\frac{1}{d}-1)}-a];\quad c > 0,\  0 < d < 1 \\
		a &= \log{(\frac{1}{b}-1)}; \quad 0 < b < 1
	\end{flalign}
\end{subequations}
$m_{max}$ is the maximum output level which can be a number between 0 to 1; $x$ is the knowledge gain potential; $b$ determines the minimum output level when knowledge gain potential is 0; $c$ represents the $x$ required for $m_{max}$ to reach $d$. 

In terms of how we compute the knowledge gain potential, we can either use average action value or relative action value. Average action value requires calibration and is not adaptive to fundamental changes in the system like when one sensors is covered or disconnected. 

Relative action value is used to reduce the difficulty of tuning for different actuators as different actuators have different inherent level of action values range. This relative action value is calculated by dividing the squared action value of this time step by average squared action values over the previous $W$ time steps as shown in \Cref{eqn:action_val_set,eqn:avg_action_val,eqn:relative_action_val}. This means that even if there is a fundamental change to the system, it can still adapt over time.  

\begin{equation}\label{eqn:action_val_set}
	Q_z = [q_{z-W}, q_{z-W-1}, ... , q_{z-1}]
\end{equation}
$Q_z$ is a set of previous $W$ action values $q$ at time step $z$; $W$ is the window size and it is a constant.
\begin{equation}\label{eqn:avg_action_val}
	\overline{q^2_z} = \frac{1}{W}\displaystyle\sum_{i=z-W}^{z-1} q_i^2
\end{equation}
$\overline{q^2_z}$ is the average squared action value at time step $z$.
\begin{equation}\label{eqn:relative_action_val}
	\widehat{q^2_z} = \frac{q_t^2}{max(\overline{q^2_z}, \nu)}
\end{equation}
$\widehat{q^2_z}$ is the relative action value; $q_z^2$ is the current action value; $nu$ is the sensitivity constant.

One consequence of this formulation is that the denominator will become too small over time and a very small increase in action value can trigger large activation. This can be a desirable feature as it creates a system that periodically wakes up in hope to attract visitors when there is nothing to be learnt for long time. This sensitivity level during low learning period can be adjusted by changing the value of $\nu$. On the other hand the window size $W$ affects how quickly the system can adapt to a new ambient environment. 


\section{Multi-Node CBLA System}

One option for controlling a larger system of sensors and actuators is to control them centrally through a single CBLA engine.  However, if all output and input variables are grouped into one node, the sensorimotor state will become very large and it might take a very long time for the system to converge and react to changes. Therefore, subsets of related variables are grouped into nodes. Each node runs on its own CBLA engine in parallel.  While our implementation uses a centralized high-level processor to compute these parallel CBLA engines, this architecture also allows for distribution of the software agents to local processors.

\subsection{Sensorimotor Context of a CBLA Node}

There are several options for grouping sensors and actuators into nodes.  One approach is to group actuators and their associated proprioceptive sensors by function, because these relationships are the easiest to model. In addition, if multiple actuators work together to perform one function that directly affects one sensor, they should be grouped as a node because their relationship must be considered in order produce a prediction model for the sensor input. However, grouping simply by function cannot capture environmental changes and occupants' interaction effectively. 

Another approach is to group sensor input and actuator output variables by spatial proximity. Since environmental changes and occupant interaction are likely to affect components that are close together physically, this will allow the system to capture those dynamics more effectively. 

However, all sensor input and actuator output variables associated with a particular node are updated once per loop. This means that the loop speed is limited by the slowest actuator or sensor within the node. For instance, an SMA-actuated Fin node has heating and cooling cycle time of about 12 seconds, while an LED can be turned on and off many degrees of magnitude faster. This is very inefficient and limiting for the fast components. Therefore, components running at different speeds should be grouped into different nodes. This allows each node to run at its own speed, independently from the other nodes. 

In our implementation, each node is constructed of components that are related functionally and proximally. For instance, LED output and ambient light sensor are in one node; Fin motion and accelerometer are in one node. Different nodes run at different frequencies and capture the dynamics of the system at different time scales. We limited the number of output per node to one. This structure keeps the system's dimensionality low and flexible. This also allows us to optimize the CBLA loop rate for each type of actuators as the bandwidth is mainly constrained by the bandwidth imposed by the actuators. Also, no more than one node can control one physical actuator since this will result in conflicts. 

\subsection{Inter-Node Connections}

Nevertheless, if the nodes perform learning independently, the CBLA does not actually capture the dynamics of the entire system. To integrate the nodes into one coherent entity, we have used shared sensors and virtual inputs.

By sharing input variables that share the same sensor, system changes effected by one node can be detected by another node indirectly. For instance, two adjacent nodes might share a single IR Proximity sensor. This way, change in the environment can be seen by both nodes. However, when two Nodes share one input variable, if the action of the actuator does not affect the shared sensor, information about that event is not transferred to the other Node. 

Another method is to treat output signal from a node as input to other nodes. We called this virtual input. This way environmental changes happening to another node can be detected by other nodes indirectly in the time steps afterwards. The idea is that external disturbance, like a user standing in front of the sensor of a node, may be represent as an output signal originated from that node, which may be measured by another node. This gives the other node information about the fact that there is someone nearby despite being somewhat delayed. \Cref{fig:Indirect input diagram} illustrates how Node 2 can detect changes in the environment that is not covered by its own sensors through Node 1.


\begin{figure}[!htbp]
	\centering
	\includegraphics[height=0.20 \textheight]{"fig/cbla/Indirect input diagram"}
	\caption[Illustration explaining how virtual inputs may be used to detect changes outside of a node's sensorimotor context]{An example of illustrating how Node 2 can indirectly detect the presence of a user through Node 1. The presence of the user may affect the output of Node 1. Since Node 2 takes the output of Node 1 as input, Node 2 would in effect respond to the presence of the user indirectly despite not having a sensor that can detect the user in its sensorimotor context.}
	\label{fig:Indirect input diagram}
\end{figure}

\FloatBarrier  

