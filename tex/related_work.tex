\chapter{Related Work} \label{chap:related_work}

\section{Interactive Arts}
Interactive arts can be categorized as Static, Dynamic-Passive, Dynamic-Interactive, and Dynamic-Interactive (varying) based on the degree of the interaction between the art works and the viewers \cite{Edmonds2004}. Dynamic-Interactive systems give the human viewers an active role in defining the behaviours of the system. This category introduces an agent that modifies the specifications of the art object. This additional unpredictability introduces a new dimension of complexity to the behaviours of the system. 
In \cite{Drummond2009}, Drummond examined the conceptual elements of interactive musical arts. For an interactive system to move beyond being simply a complex musical instrument with a direct reactive mapping from inputs to generation of sound, it must possess some level of autonomy in the composition and generation of music. In addition, the interactivity should be bilateral; the performer influences the music and the music influences the performer. These concepts can easily be extended to visual and kinetic arts. Visual-based interactive systems such as the Iamascope in Beta\_space \cite{Costello2005} and audio-based systems such as Variations \cite{Wands} engaged the participants by allowing them to directly affect the output of the system. Works in interactive architecture [1] [8] try to provide a responsive and immersive environment where the viewers can feel as if they belong to the system. 
However, most of these works are the non-varying type of Dynamic-Interactive system, as their responsive behaviours do not change. Over a longer term, the system will become more predictable and its effectiveness in engaging the users will consequently decrease. In this work, we aim to create a varying interactive artwork by emulating the characteristics of living organisms such as curiosity and learning \cite{Beesley2012}. 

\section{Artificial Life}

To emulate life-like behaviours, one can start by observing how human beings behave. \cite{Dragan2015}, \cite{AncaDraga2014} modelled how human beings convey or mask their intentions through movement and applied these models on a humanoid robot. Similarly, \cite{Gielniak2013} focuses on making the robot's motion more understandable by emulating the coordinated effects of human joints. Those studies focus their attention on making the intent of the robots clear. In contrast, our objective is to make robots more engaging and life-like, where unpredictability might be a desirable quality. For instance, \cite{AncaDraga2014} showed that the robot's perceived intelligence increased when the participants believed that the robot was intentionally deceptive. Our work investigates whether unpredictable behaviours emerging from the learning process will appear more life-like and engaging.
One of the open questions in artificial life research is whether we can demonstrate the emergence of intelligence and mind \cite{Bedau2000}, examined in projects such as the Petting Zoo by Minimaforms \cite{Minimaforms} and Mind Time Machine \cite{Ikegami2013}.   The idea of emergence of structure and consciousness is explored in many previous works in the field of developmental robotics \cite{Lungarella2003} \cite{Asada2009} \cite{Kompella2014}.  Oudeyer et al. developed a learning mechanism called Intelligent Adaptive Curiosity (IAC) \cite{Oudeyer2007}, a reinforcement learning algorithm with the objective of maximizing learning progress. In his experiments, he showed that an agent would tend to explore state-space that is neither too predictable nor too random, mimicking the intrinsic human drive of curiosity, which continually tries to explore areas that have the highest potential for learning new knowledge. However, previous work did not cover how the IAC might be scaled to a distributed system with a large sensorimotor space. 

\section{Developmental Robotics}

\section{Machine Learning}

% discuss linear regression and LASSO

\section[Intrinsic Adaptive Curiosity] {Intrinsic Adaptive Curiosity\footnote{An early version of this section has been published at IROS 2015 \cite{Chan2015} }}

In \cite{Oudeyer2007}, Oudeyer's goal was to develop a robot that is capable of life-long learning. The robot makes sense of the mapping between its actuators and sensors through continuous self-experimentation, without explicit instruction from human experts. Fundamentally, the IAC is a reinforcement learning algorithm with an objective to maximize learning progress. Learning progress is defined as the reduction in prediction error. In other words, the agent seeks to explore the region of the sensorimotor space that leads to the highest improvement in prediction error. As a result, the algorithm avoids learning in the parts of sensorimotor space that are either too random or too predictable. This formulation leads to continual change in behaviour over time, as regions of the state space that are initially interesting become less interesting as the system becomes more knowledgeable about them. 

The system consists of two learning mechanisms, the Classic Machine learner (classM) and the Meta Machine learner (metaM).  Based on the context (the sensors' inputs) and the action (the actuators' outputs), classM computes a prediction of the consequence, i.e., the resultant sensors' inputs at the next time step. Then, it implements the action and compares the actual consequence with the prediction and modifies its model in order to reduce the error in the subsequent prediction. The Meta Machine learner (metaM) predicts the error of classM. In other words, it estimates how accurately classM is able predict the consequence. The actual prediction error is then fed back to the metaM, and metaM modifies its estimate of prediction error of the newly modified classM. This change in the prediction error is recorded at each time step in the knowledge gain assessor (KGA). The KGA then computes the expected learning progress by calculating the change in error estimation. This expected learning progress is used as the reward, $R$, as classM prediction gets better in a given area of sensorimotor space, the expected learning progress diminishes and the reward for exploring that area gets smaller. Higher rewards in other as-yet unexplored regions encourage the algorithm to move on, to satisfy its "curiosity.". 

In Oudeyer's paper \cite{Oudeyer2007}, one important feature is the idea of regional experts. Each region collects exemplars of similar sensorimotor context, and has an expert that is trained on the exemplars in the region. Exemplars are the training data for the prediction model and they are collected as the system selects actions and observes the consequences. The ``features'' are a vector of the sensory inputs $S(t)$, and a vector of selected actions $M(t)$ at time $t$; and the ``labels'' are the resultant sensory inputs $S(t+1)$ at time $t+1$. The regional experts constrain the estimate of learning progress within their respective sensorimotor contexts $SM(t)$ which is a concatenation of the vectors $S(t)$ and $M(t)$. This is important because it allows each expert to use a simpler model (essentially a local linearizion), as it covers only a small region of the state space.  

The following are the steps for the learning algorithm:

\begin{enumerate}
	\item \label{learning_start} Read sensory inputs $S(t)$
	\item Select action $M(t)$ with the highest predicted learning progress $R(SM(t))$ based on $\epsilon$-greedy selection policy
	\item Consult expert specialized in the relevant sensorimotor context $SM(t)$ to predict the expected sensory inputs $S(t+1)'$
	\item Perform action $M(t)$
	\item Observe actual sensory inputs $S(t+1)$ and add to the expert's training set
	\item Compute prediction error $e(t+1) = |S(t+1) - S(t+1)'|$
	\item Compute the change in prediction error $R(SM(t)) = -[e(t+1) - e(t)]$
	\item Update the learning progress for the sensorimotor context $SM(t)$ based on $R(SM(t))$
	\item \label{learning_loop} Repeat \ref{learning_start} to \ref{learning_loop} indefinitely
\end{enumerate}



\section{User Experience Survey}
